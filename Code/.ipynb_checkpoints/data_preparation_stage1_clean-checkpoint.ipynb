{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7544c97a",
   "metadata": {},
   "source": [
    "# OHIE Data Preparation – Stage 1\n",
    "\n",
    "This notebook performs the *data preparation stage* for my PhD writing sample using the Oregon Health Insurance Experiment (OHIE).  \n",
    "The goal here is **only** to clean and structure the data, not to run the causal ML or main econometric models.\n",
    "\n",
    "Concretely, this notebook:\n",
    "\n",
    "1. Loads the four OHIE source files (baseline 0m survey, 12m survey, descriptive/lottery list, and state program records).\n",
    "2. Harmonizes identifiers and data types and merges to a person-level file.\n",
    "3. Defines the instrument `Z` (lottery win) and treatment `W` (ever enrolled in Medicaid), plus the main outcome and covariate blocks.\n",
    "4. Constructs an analysis sample (responded to both 0m and 12m).\n",
    "5. Prefixes variables as `Y_` (outcomes) and `X_` (baseline covariates) so that the later causal ML code can treat them systematically.\n",
    "6. Performs careful missing-data diagnostics and imputes covariates using an iterative tree-based imputer with missingness flags.\n",
    "7. Constructs a numeric income measure and a catastrophic expenditure indicator.\n",
    "8. Saves a clean intermediate dataset to disk for use in all downstream empirical work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e73ea",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab165e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:,.3f}\".format)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_DIR = Path(\"../Data_Used\")\n",
    "assert DATA_DIR.exists(), f\"DATA_DIR does not exist: {DATA_DIR.resolve()}\"\n",
    "DATA_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d10d3",
   "metadata": {},
   "source": [
    "## 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_unique_key(df: pd.DataFrame, key):\n",
    "    \"\"\"Assert that `key` is a unique row identifier in df.\"\"\"\n",
    "    if isinstance(key, str):\n",
    "        key = [key]\n",
    "    dup = df.duplicated(subset=key).sum()\n",
    "    if dup > 0:\n",
    "        raise ValueError(f\"Key {key} is not unique: {dup} duplicate rows found.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def summarize_missingness(df: pd.DataFrame, cols, name: str, max_rows: int = 30):\n",
    "    \"\"\"Print simple missingness summary for a list of columns.\"\"\"\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    miss = df[cols].isna().mean().sort_values(ascending=False)\n",
    "    print(f\"\\nMissingness summary for {name} (share of rows that are NA):\")\n",
    "    display(miss.head(max_rows))\n",
    "\n",
    "\n",
    "def quick_tabulate(df: pd.DataFrame, col: str):\n",
    "    \"\"\"Convenience wrapper for value_counts with NA visible.\"\"\"\n",
    "    print(f\"\\nValue counts for {col}:\")\n",
    "    display(df[col].value_counts(dropna=False).to_frame(\"n\").assign(\n",
    "        pct=lambda s: s[\"n\"] / s[\"n\"].sum()\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88a88d",
   "metadata": {},
   "source": [
    "## 3. Load 12‑month survey (outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12m survey – outcome block\n",
    "col_keep_y = [\n",
    "    # --- A. Key & filter variables ---\n",
    "    \"person_id\",       # Merge key\n",
    "    \"returned_12m\",    # 12m response indicator\n",
    "    \"weight_12m\",      # Official OHIE 12m nonresponse weight\n",
    "\n",
    "    # --- B. Primary financial outcomes ---\n",
    "    # 1. Medical debt\n",
    "    \"cost_any_owe_12m\",   # [0/1] Any medical debt\n",
    "    \"cost_tot_owe_12m\",   # [Num] Total medical debt\n",
    "\n",
    "    # 2. Financial distress\n",
    "    \"cost_borrow_12m\",    # [0/1] Borrowed / skipped bills to pay medical expenses\n",
    "    \"cost_refused_12m\",   # [0/1] Refused care due to non‑payment\n",
    "\n",
    "    # 3. Out‑of‑pocket spending (aggregate + breakdown)\n",
    "    \"cost_tot_oop_12m\",   # [Num] Total OOP over 12m\n",
    "    \"cost_any_oop_12m\",   # [0/1] Any OOP spending\n",
    "    \"hhinc_cat_12m\",      # [Cat] Household income category (used for catastrophic indicator)\n",
    "\n",
    "    # Detailed OOP by service type\n",
    "    \"cost_doc_oop_12m\",   # [Num] OOP on doctor visits\n",
    "    \"cost_er_oop_12m\",    # [Num] OOP on ER visits\n",
    "    \"cost_rx_oop_12m\",    # [Num] OOP on prescriptions\n",
    "    \"cost_oth_oop_12m\",   # [Num] OOP on other care\n",
    "]\n",
    "\n",
    "y_path = DATA_DIR / \"oregonhie_survey12m_vars.dta\"\n",
    "df_y = pd.read_stata(\n",
    "    y_path,\n",
    "    columns=col_keep_y,\n",
    "    convert_categoricals=False,\n",
    "    preserve_dtypes=True,\n",
    ")\n",
    "\n",
    "if df_y[\"person_id\"].isna().any():\n",
    "    raise ValueError(\"df_y has missing person_id values.\")\n",
    "\n",
    "df_y[\"person_id\"] = df_y[\"person_id\"].astype(\"int64\")\n",
    "assert_unique_key(df_y, \"person_id\")\n",
    "\n",
    "print(\"df_y shape:\", df_y.shape)\n",
    "df_y.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f3c88",
   "metadata": {},
   "source": [
    "## 4. Load baseline 0m survey (covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0m survey – baseline covariate block\n",
    "col_keep_x = [\n",
    "    # --- A. Key & filter ---\n",
    "    \"person_id\",        # Merge key\n",
    "    \"returned_0m\",      # Baseline survey response\n",
    "    \"surv_lang_0m\",     # Survey language\n",
    "\n",
    "    # --- B. Baseline health need & utilization ---\n",
    "    \"needmet_med_0m\",   # [0/1] Got all needed medical care (unmet need)\n",
    "    \"needmet_rx_0m\",    # [0/1] Got all needed medications\n",
    "    \"need_rx_0m\",       # [0/1] Needed medication\n",
    "    \"need_med_0m\",      # [0/1] Needed medical care\n",
    "\n",
    "    \"rx_num_mod_0m\",    # [Num] # of distinct prescriptions\n",
    "    \"doc_num_mod_0m\",   # [Num] # doctor visits last 6m\n",
    "    \"er_num_mod_0m\",    # [Num] # ER visits last 6m\n",
    "    \"hosp_num_mod_0m\",  # [Num] # hospitalizations last 6m\n",
    "\n",
    "    \"ins_months_0m\",    # [0–6] # months insured in last 6m\n",
    "\n",
    "    # --- C. Baseline health status ---\n",
    "    \"health_gen_0m\",    # [1–5] Self‑rated health\n",
    "    \"baddays_phys_0m\",  # [0–30] Days physical health \"not good\"\n",
    "    \"baddays_ment_0m\",  # [0–30] Days mental health \"not good\"\n",
    "    \"health_chg_0m\",    # [1–3] Health better/same/worse vs 1 year ago\n",
    "\n",
    "    # --- D. Diagnosed conditions ---\n",
    "    \"dia_dx_0m\",        # [0/1] Diabetes\n",
    "    \"ast_dx_0m\",        # [0/1] Asthma\n",
    "    \"hbp_dx_0m\",        # [0/1] Hypertension\n",
    "    \"emp_dx_0m\",        # [0/1] Emphysema/COPD\n",
    "    \"chf_dx_0m\",        # [0/1] Congestive heart failure\n",
    "    \"dep_dx_0m\",        # [0/1] Depression / anxiety\n",
    "\n",
    "    # --- E. Demographics ---\n",
    "    \"female_0m\",        # [0/1] Female\n",
    "    \"birthyear_0m\",     # [Year] Birth year (used for baseline age)\n",
    "    \"edu_0m\",           # [Cat] Education\n",
    "\n",
    "    # --- F. Race & ethnicity ---\n",
    "    \"race_hisp_0m\",     # [0/1] Hispanic\n",
    "    \"race_white_0m\",    # [0/1] White\n",
    "    \"race_black_0m\",    # [0/1] Black\n",
    "    \"race_amerindian_0m\",  # [0/1] American Indian / Alaska Native\n",
    "    \"race_asian_0m\",       # [0/1] Asian\n",
    "    \"race_pacific_0m\",     # [0/1] Native Hawaiian / Pacific Islander\n",
    "    \"race_other_qn_0m\",    # [0/1] Other race\n",
    "\n",
    "    # --- G. Employment & household ---\n",
    "    \"employ_0m\",        # [0/1] Employed\n",
    "    \"employ_hrs_0m\",    # [Cat] Hours worked / week\n",
    "    \"hhinc_cat_0m\",     # [Cat] Baseline HH income category\n",
    "    \"hhsize_0m\",        # [Num] Household size\n",
    "    \"num19_0m\",         # [Num] # children under 19\n",
    "\n",
    "    # --- H. Baseline financial variables ---\n",
    "    \"cost_any_oop_0m\",  # [0/1] Any OOP at baseline\n",
    "    \"cost_borrow_0m\",   # [0/1] Borrowed / skipped bills for medical care\n",
    "    \"cost_any_owe_0m\",  # [0/1] Owe any medical debt\n",
    "    \"cost_tot_owe_0m\",  # [Num] Total baseline medical debt\n",
    "    \"cost_refused_0m\",  # [0/1] Refused care for non‑payment\n",
    "    # If `cost_tot_oop_0m` exists, we will use it to build a cleaned OOP measure.\n",
    "    # Otherwise, we will back out total from any available OOP components.\n",
    "    \"cost_tot_oop_0m\",\n",
    "\n",
    "    # --- I. Location (from 0m / list) ---\n",
    "    \"zip_msa_list\",     # [0/1] MSA indicator (urban vs rural)\n",
    "]\n",
    "\n",
    "x_path = DATA_DIR / \"oregonhie_survey0m_vars.dta\"\n",
    "df_x = pd.read_stata(\n",
    "    x_path,\n",
    "    columns=[c for c in col_keep_x if c is not None],\n",
    "    convert_categoricals=False,\n",
    "    preserve_dtypes=True,\n",
    ")\n",
    "\n",
    "if df_x[\"person_id\"].isna().any():\n",
    "    raise ValueError(\"df_x has missing person_id values.\")\n",
    "\n",
    "df_x[\"person_id\"] = df_x[\"person_id\"].astype(\"int64\")\n",
    "assert_unique_key(df_x, \"person_id\")\n",
    "\n",
    "# Construct a cleaned total OOP measure at baseline.\n",
    "if \"cost_tot_oop_0m\" in df_x.columns:\n",
    "    df_x[\"cost_tot_oop_correct_0m\"] = df_x[\"cost_tot_oop_0m\"]\n",
    "else:\n",
    "    # Fallback: if detailed OOP components exist (rare), sum them.\n",
    "    oop_components = [c for c in [\n",
    "        \"cost_doc_oop_0m\", \"cost_er_oop_0m\", \"cost_rx_oop_0m\", \"cost_oth_oop_0m\"\n",
    "    ] if c in df_x.columns]\n",
    "    if oop_components:\n",
    "        df_x[\"cost_tot_oop_correct_0m\"] = df_x[oop_components].sum(axis=1)\n",
    "    else:\n",
    "        df_x[\"cost_tot_oop_correct_0m\"] = np.nan\n",
    "\n",
    "print(\"df_x shape:\", df_x.shape)\n",
    "df_x.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a46d6b",
   "metadata": {},
   "source": [
    "## 5. Load descriptive / lottery list file (instrument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_keep_iv = [\n",
    "    \"person_id\",\n",
    "    \"household_id\",\n",
    "    \"treatment\",      # [0/1] Lottery win (instrument Z)\n",
    "    \"numhh_list\",     # [Num] # of people on lottery list in household\n",
    "    \"zip_msa_list\",   # [0/1] MSA indicator (urban vs rural)\n",
    "    \"female_list\",    # [0/1] Gender at sign‑up (backup)\n",
    "    \"birthyear_list\", # [Year] Birth year at sign‑up (backup)\n",
    "]\n",
    "\n",
    "iv_path = DATA_DIR / \"oregonhie_descriptive_vars.dta\"\n",
    "df_iv = pd.read_stata(\n",
    "    iv_path,\n",
    "    columns=col_keep_iv,\n",
    "    convert_categoricals=False,\n",
    "    preserve_dtypes=True,\n",
    ")\n",
    "\n",
    "if df_iv[\"person_id\"].isna().any():\n",
    "    raise ValueError(\"df_iv has missing person_id values.\")\n",
    "if df_iv[\"household_id\"].isna().any():\n",
    "    raise ValueError(\"df_iv has missing household_id values.\")\n",
    "\n",
    "df_iv[\"person_id\"] = df_iv[\"person_id\"].astype(\"int64\")\n",
    "df_iv[\"household_id\"] = df_iv[\"household_id\"].astype(\"int64\")\n",
    "assert_unique_key(df_iv, \"person_id\")\n",
    "\n",
    "print(\"df_iv shape:\", df_iv.shape)\n",
    "df_iv.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a644759",
   "metadata": {},
   "source": [
    "## 6. Load state program file (treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9269b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_keep_w = [\n",
    "    \"person_id\",\n",
    "    # Ever enrolled in Medicaid (OHP Standard) by 30 Sept 2009 – main treatment\n",
    "    \"ohp_all_ever_firstn_30sep2009\",\n",
    "    # Months enrolled (intensity)\n",
    "    \"ohp_all_mo_firstn_30sep2009\",\n",
    "]\n",
    "\n",
    "w_path = DATA_DIR / \"oregonhie_stateprograms_vars.dta\"\n",
    "df_w = pd.read_stata(\n",
    "    w_path,\n",
    "    columns=col_keep_w,\n",
    "    convert_categoricals=False,\n",
    "    preserve_dtypes=True,\n",
    ")\n",
    "\n",
    "if df_w[\"person_id\"].isna().any():\n",
    "    raise ValueError(\"df_w has missing person_id values.\")\n",
    "\n",
    "df_w[\"person_id\"] = df_w[\"person_id\"].astype(\"int64\")\n",
    "assert_unique_key(df_w, \"person_id\")\n",
    "\n",
    "print(\"df_w shape:\", df_w.shape)\n",
    "df_w.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa246bd5",
   "metadata": {},
   "source": [
    "## 7. Merge all sources and define instrument (Z) and treatment (W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge starting from the universe of lottery applicants (df_iv)\n",
    "df_merged = (\n",
    "    df_iv\n",
    "    .merge(df_x, on=\"person_id\", how=\"left\", validate=\"1:1\")\n",
    "    .merge(df_y, on=\"person_id\", how=\"left\", validate=\"1:1\")\n",
    "    .merge(df_w, on=\"person_id\", how=\"left\", validate=\"1:1\")\n",
    ")\n",
    "\n",
    "print(f\"After merge: {df_merged.shape[0]:,} rows × {df_merged.shape[1]} columns\")\n",
    "\n",
    "# Rename to follow econometric convention (Z,W)\n",
    "df_merged = df_merged.rename(columns={\n",
    "    \"treatment\": \"Z_lottery\",\n",
    "    \"ohp_all_ever_firstn_30sep2009\": \"W_medicaid\",\n",
    "    \"ohp_all_mo_firstn_30sep2009\": \"W_medicaid_months\",\n",
    "})\n",
    "\n",
    "# People who never enrolled have missing W_medicaid; treat as 0\n",
    "df_merged[\"W_medicaid\"] = df_merged[\"W_medicaid\"].fillna(0).astype(\"int8\")\n",
    "df_merged[\"W_medicaid_months\"] = df_merged[\"W_medicaid_months\"].fillna(0)\n",
    "\n",
    "quick_tabulate(df_merged, \"Z_lottery\")\n",
    "quick_tabulate(df_merged, \"W_medicaid\")\n",
    "\n",
    "# Quick check: Z must be binary and defined for everyone in the lottery list\n",
    "unique_z = sorted(df_merged[\"Z_lottery\"].dropna().unique())\n",
    "print(\"\\nUnique values of Z_lottery:\", unique_z)\n",
    "assert set(unique_z).issubset({0, 1}), \"Z_lottery is not binary\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8221f223",
   "metadata": {},
   "source": [
    "## 8. Attrition analysis and use of 12m weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ATTRITION ANALYSIS & WEIGHT DIAGNOSTICS ===\")\n",
    "\n",
    "# Response by instrument status\n",
    "attrition_table = df_merged.groupby(\"Z_lottery\")[\"returned_12m\"].agg([\"mean\", \"count\", \"sum\"])\n",
    "attrition_table.columns = [\"Response_Rate\", \"Total_N\", \"Responders_N\"]\n",
    "print(\"\\n1. Response rates by lottery status:\")\n",
    "display(attrition_table)\n",
    "\n",
    "# Two-sample z-test for equality of response rates across Z\n",
    "z_stat, p_attrition = proportions_ztest(\n",
    "    count=attrition_table[\"Responders_N\"],\n",
    "    nobs=attrition_table[\"Total_N\"],\n",
    ")\n",
    "diff_pp = (attrition_table.loc[1, \"Response_Rate\"] - attrition_table.loc[0, \"Response_Rate\"]) * 100\n",
    "print(f\"\\nDifference in response rates (Z=1 − Z=0): {diff_pp:+.2f} pp\")\n",
    "print(f\"z = {z_stat:.3f}, p = {p_attrition:.4f}\")\n",
    "\n",
    "# Construct a normalized nonresponse weight based on the official weight_12m\n",
    "df_merged[\"weight_attrition\"] = np.where(\n",
    "    df_merged[\"returned_12m\"] == 1,\n",
    "    df_merged[\"weight_12m\"],\n",
    "    np.nan,\n",
    ")\n",
    "mean_w = df_merged[\"weight_attrition\"].mean(skipna=True)\n",
    "df_merged[\"weight_attrition\"] = df_merged[\"weight_attrition\"] / mean_w\n",
    "\n",
    "print(\"\\n2. Distribution of attrition weights among 12m respondents:\")\n",
    "weight_stats = (\n",
    "    df_merged.loc[df_merged[\"returned_12m\"] == 1]\n",
    "    .groupby(\"Z_lottery\")[\"weight_attrition\"]\n",
    "    .agg([\"mean\", \"min\", \"max\"])\n",
    ")\n",
    "display(weight_stats)\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "resp = df_merged[\"returned_12m\"] == 1\n",
    "w_z1 = df_merged.loc[resp & (df_merged[\"Z_lottery\"] == 1), \"weight_attrition\"]\n",
    "w_z0 = df_merged.loc[resp & (df_merged[\"Z_lottery\"] == 0), \"weight_attrition\"]\n",
    "t_stat, p_weight = ttest_ind(w_z1, w_z0, alternative=\"greater\")\n",
    "print(f\"\\nTest of higher weights for Z=1 vs Z=0 among respondents: p = {p_weight:.4f}\")\n",
    "print(f\"Z=1 mean weight: {w_z1.mean():.2f}, Z=0 mean weight: {w_z0.mean():.2f}\")\n",
    "\n",
    "# Extreme-weight diagnostics\n",
    "extreme_thresh = 5\n",
    "extreme_pct = (\n",
    "    df_merged.loc[resp, \"weight_attrition\"].gt(extreme_thresh).mean() * 100\n",
    ")\n",
    "print(f\"\\n3. Extreme weights (> {extreme_thresh}): {extreme_pct:.2f}% of 12m respondents\")\n",
    "\n",
    "# Optional: simple visualization (for interactive use)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(\n",
    "    data=df_merged.loc[resp],\n",
    "    x=\"weight_attrition\",\n",
    "    hue=\"Z_lottery\",\n",
    "    element=\"step\",\n",
    "    common_norm=False,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Weight distribution by lottery status (respondents)\")\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df_merged.loc[resp],\n",
    "    x=\"Z_lottery\",\n",
    "    y=\"weight_attrition\",\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_xticklabels([\"Lost\", \"Won\"])\n",
    "ax2.set_title(\"Weight outliers\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "attrition_diagnostic = {\n",
    "    \"response_rate_diff_pp\": float(diff_pp),\n",
    "    \"attrition_p_value\": float(p_attrition),\n",
    "    \"weight_mean_z1\": float(w_z1.mean()),\n",
    "    \"weight_mean_z0\": float(w_z0.mean()),\n",
    "    \"extreme_weight_pct\": float(extreme_pct),\n",
    "}\n",
    "attrition_diagnostic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d6056",
   "metadata": {},
   "source": [
    "## 9. Define analysis sample and main X/Y blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb4bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep individuals who answered both baseline (0m) and 12m surveys\n",
    "df_analysis = df_merged.loc[\n",
    "    (df_merged[\"returned_0m\"] == 1) &\n",
    "    (df_merged[\"returned_12m\"] == 1)\n",
    "].copy()\n",
    "\n",
    "print(\n",
    "    f\"Analysis sample (responded to 0m and 12m): \"\n",
    "    f\"{df_analysis.shape[0]:,} rows × {df_analysis.shape[1]} columns\"\n",
    ")\n",
    "\n",
    "# Main outcome block (raw column names before prefixing)\n",
    "Y_raw = [\n",
    "    \"cost_any_owe_12m\",\n",
    "    \"cost_tot_owe_12m\",\n",
    "    \"cost_borrow_12m\",\n",
    "    \"cost_refused_12m\",\n",
    "    \"cost_tot_oop_12m\",\n",
    "    \"cost_any_oop_12m\",\n",
    "    \"hhinc_cat_12m\",\n",
    "    \"cost_doc_oop_12m\",\n",
    "    \"cost_er_oop_12m\",\n",
    "    \"cost_rx_oop_12m\",\n",
    "    \"cost_oth_oop_12m\",\n",
    "]\n",
    "\n",
    "# Main covariate block (baseline + lottery‑list covariates)\n",
    "X_raw = [\n",
    "    \"surv_lang_0m\",\n",
    "    \"needmet_med_0m\",\n",
    "    \"needmet_rx_0m\",\n",
    "    \"need_rx_0m\",\n",
    "    \"need_med_0m\",\n",
    "    \"rx_num_mod_0m\",\n",
    "    \"doc_num_mod_0m\",\n",
    "    \"er_num_mod_0m\",\n",
    "    \"hosp_num_0m\",\n",
    "    \"hosp_num_mod_0m\",   # keep both in case only one is present\n",
    "    \"ins_months_0m\",\n",
    "    \"health_gen_0m\",\n",
    "    \"baddays_phys_0m\",\n",
    "    \"baddays_ment_0m\",\n",
    "    \"health_chg_0m\",\n",
    "    \"dia_dx_0m\",\n",
    "    \"ast_dx_0m\",\n",
    "    \"hbp_dx_0m\",\n",
    "    \"emp_dx_0m\",\n",
    "    \"chf_dx_0m\",\n",
    "    \"dep_dx_0m\",\n",
    "    \"female_0m\",\n",
    "    \"birthyear_0m\",\n",
    "    \"edu_0m\",\n",
    "    \"race_hisp_0m\",\n",
    "    \"race_white_0m\",\n",
    "    \"race_black_0m\",\n",
    "    \"race_amerindian_0m\",\n",
    "    \"race_asian_0m\",\n",
    "    \"race_pacific_0m\",\n",
    "    \"race_other_qn_0m\",\n",
    "    \"employ_0m\",\n",
    "    \"employ_hrs_0m\",\n",
    "    \"hhinc_cat_0m\",\n",
    "    \"hhsize_0m\",\n",
    "    \"num19_0m\",\n",
    "    \"cost_tot_owe_0m\",\n",
    "    \"cost_borrow_0m\",\n",
    "    \"cost_any_owe_0m\",\n",
    "    \"cost_refused_0m\",\n",
    "    \"cost_any_oop_0m\",\n",
    "    \"cost_tot_oop_correct_0m\",\n",
    "    \"zip_msa_list\",\n",
    "]\n",
    "\n",
    "# Keep only the X/Y columns that are actually present\n",
    "Y_raw = [c for c in Y_raw if c in df_analysis.columns]\n",
    "X_raw = [c for c in X_raw if c in df_analysis.columns]\n",
    "\n",
    "print(\"\\nIncluded Y columns:\", Y_raw)\n",
    "print(\"\\nIncluded X columns:\", X_raw)\n",
    "\n",
    "summarize_missingness(df_analysis, Y_raw, \"Y (outcomes)\")\n",
    "summarize_missingness(df_analysis, X_raw, \"X (baseline covariates)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49311d5",
   "metadata": {},
   "source": [
    "## 10. Prefix X_ and Y_ variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {}\n",
    "\n",
    "for col in X_raw:\n",
    "    rename_map[col] = f\"X_{col}\"\n",
    "\n",
    "for col in Y_raw:\n",
    "    rename_map[col] = f\"Y_{col}\"\n",
    "\n",
    "df_analysis_sample = df_analysis.rename(columns=rename_map).copy()\n",
    "\n",
    "X_cols = [f\"X_{c}\" for c in X_raw]\n",
    "Y_cols = [f\"Y_{c}\" for c in Y_raw]\n",
    "\n",
    "print(f\"After renaming, analysis sample: {df_analysis_sample.shape[0]:,} rows\")\n",
    "print(\"\\nFirst few X variables:\", X_cols[:10])\n",
    "print(\"First few Y variables:\", Y_cols[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d5ef6",
   "metadata": {},
   "source": [
    "## 11. Income mapping and catastrophic expenditure indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ebc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map OHIE 12m income categories to numeric midpoints (2008 dollars)\n",
    "income_map_12m = {\n",
    "    1: 0,       # \"$0\"\n",
    "    2: 1250,    # \"$1 to $2,500\"\n",
    "    3: 3750,    # \"$2,501 to $5,000\"\n",
    "    4: 6250,    # \"$5,001 to $7,500\"\n",
    "    5: 8750,    # \"$7,501 to $10,000\"\n",
    "    6: 11250,   # \"$10,001 to $12,500\"\n",
    "    7: 13750,   # \"$12,501 to $15,000\"\n",
    "    8: 16250,   # \"$15,001 to $17,500\"\n",
    "    9: 18750,   # \"$17,501 to $20,000\"\n",
    "    10: 21250,  # \"$20,001 to $22,500\"\n",
    "    11: 23750,  # \"$22,501 to $25,000\"\n",
    "    12: 26250,  # \"$25,001 to $27,500\"\n",
    "    13: 28750,  # \"$27,501 to $30,000\"\n",
    "    14: 31250,  # \"$30,001 to $32,500\"\n",
    "    15: 33750,  # \"$32,501 to $35,000\"\n",
    "    16: 36250,  # \"$35,001 to $37,500\"\n",
    "    17: 38750,  # \"$37,501 to $40,000\"\n",
    "    18: 41250,  # \"$40,001 to $42,500\"\n",
    "    19: 43750,  # \"$42,501 to $45,000\"\n",
    "    20: 46250,  # \"$45,001 to $47,500\"\n",
    "    21: 48750,  # \"$47,501 to $50,000\"\n",
    "    22: 60000,  # \"$50,001 or more\" (upper bin; exact value not important for our purposes)\n",
    "}\n",
    "\n",
    "if \"Y_hhinc_cat_12m\" in df_analysis_sample.columns:\n",
    "    df_analysis_sample[\"Y_income_num_12m\"] = df_analysis_sample[\"Y_hhinc_cat_12m\"].map(income_map_12m)\n",
    "    summarize_missingness(df_analysis_sample, [\"Y_income_num_12m\"], \"numeric 12m income\")\n",
    "\n",
    "# Catastrophic expenditure: OOP > 30% of income (with careful handling of zero / missing income)\n",
    "def calc_catastrophic(row):\n",
    "    income = row.get(\"Y_income_num_12m\", np.nan)\n",
    "    oop = row.get(\"Y_cost_tot_oop_12m\", np.nan)\n",
    "\n",
    "    if pd.isna(income) or pd.isna(oop):\n",
    "        return np.nan\n",
    "    if income == 0:\n",
    "        return 1 if oop > 0 else 0\n",
    "    return 1 if (oop / income) > 0.30 else 0\n",
    "\n",
    "df_analysis_sample[\"Y_catastrophic_exp_12m\"] = df_analysis_sample.apply(calc_catastrophic, axis=1)\n",
    "print(\"\\nCatastrophic expenditure rate (non-missing):\",\n",
    "      df_analysis_sample[\"Y_catastrophic_exp_12m\"].mean(skipna=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafcb0c",
   "metadata": {},
   "source": [
    "## 12. Baseline age construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d403a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct baseline age (using 2008 as reference year)\n",
    "if \"X_birthyear_0m\" in df_analysis_sample.columns:\n",
    "    df_analysis_sample[\"X_age_0m\"] = 2008 - df_analysis_sample[\"X_birthyear_0m\"]\n",
    "    print(\"Baseline age range:\",\n",
    "          df_analysis_sample[\"X_age_0m\"].min(),\n",
    "          \"to\",\n",
    "          df_analysis_sample[\"X_age_0m\"].max())\n",
    "    # Keep birthyear only for potential robustness; comment out the next line to retain it\n",
    "    df_analysis_sample = df_analysis_sample.drop(columns=[\"X_birthyear_0m\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426519a",
   "metadata": {},
   "source": [
    "## 13. Missing-data handling and imputation for X covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d763e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for baseline covariates (after X_ prefix)\n",
    "col_schema = {\n",
    "    \"binary\": [\n",
    "        \"X_need_med_0m\", \"X_needmet_med_0m\", \"X_need_rx_0m\", \"X_needmet_rx_0m\",\n",
    "        \"X_dia_dx_0m\", \"X_ast_dx_0m\", \"X_hbp_dx_0m\", \"X_emp_dx_0m\", \"X_chf_dx_0m\",\n",
    "        \"X_dep_dx_0m\", \"X_female_0m\", \"X_employ_0m\", \"X_zip_msa_list\",\n",
    "        \"X_race_hisp_0m\", \"X_race_white_0m\", \"X_race_black_0m\", \"X_race_amerindian_0m\",\n",
    "        \"X_race_asian_0m\", \"X_race_pacific_0m\", \"X_race_other_qn_0m\",\n",
    "        \"X_cost_borrow_0m\", \"X_cost_any_owe_0m\", \"X_cost_refused_0m\", \"X_cost_any_oop_0m\",\n",
    "    ],\n",
    "    \"count\": [\n",
    "        \"X_rx_num_mod_0m\", \"X_doc_num_mod_0m\", \"X_er_num_mod_0m\", \"X_hosp_num_mod_0m\",\n",
    "        \"X_hhsize_0m\", \"X_num19_0m\",\n",
    "    ],\n",
    "    \"ordinal\": [\n",
    "        \"X_surv_lang_0m\", \"X_health_gen_0m\", \"X_health_chg_0m\",\n",
    "        \"X_hhinc_cat_0m\", \"X_edu_0m\", \"X_employ_hrs_0m\",\n",
    "        \"X_baddays_phys_0m\", \"X_baddays_ment_0m\", \"X_ins_months_0m\",\n",
    "    ],\n",
    "    \"continuous\": [\n",
    "        \"X_cost_tot_owe_0m\", \"X_cost_tot_oop_correct_0m\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Keep only schema variables that are actually present in the analysis sample\n",
    "for k in col_schema:\n",
    "    col_schema[k] = [c for c in col_schema[k] if c in df_analysis_sample.columns]\n",
    "\n",
    "all_x_cols = col_schema[\"binary\"] + col_schema[\"count\"] + col_schema[\"ordinal\"] + col_schema[\"continuous\"]\n",
    "all_x_cols = [c for c in all_x_cols if c in df_analysis_sample.columns]\n",
    "\n",
    "print(f\"Total X covariates in schema: {len(all_x_cols)}\")\n",
    "summarize_missingness(df_analysis_sample, all_x_cols, \"X (schema vars)\")\n",
    "\n",
    "# Columns that actually need imputation (have any missing values)\n",
    "cols_to_impute = [c for c in all_x_cols if df_analysis_sample[c].isna().any()]\n",
    "print(f\"\\nColumns with missingness to impute: {len(cols_to_impute)}\")\n",
    "\n",
    "missing_flag_cols = []\n",
    "for col in cols_to_impute:\n",
    "    flag_col = f\"{col}_missing\"\n",
    "    df_analysis_sample[flag_col] = df_analysis_sample[col].isna().astype(\"int8\")\n",
    "    missing_flag_cols.append(flag_col)\n",
    "print(f\"Total missing-flag columns created: {len(missing_flag_cols)}\")\n",
    "\n",
    "# Keep original ranges of ordinal variables for post-imputation sanity checks\n",
    "orig_minmax = {\n",
    "    col: (\n",
    "        df_analysis_sample[col].min(skipna=True),\n",
    "        df_analysis_sample[col].max(skipna=True),\n",
    "    )\n",
    "    for col in col_schema[\"ordinal\"] if col in cols_to_impute\n",
    "}\n",
    "\n",
    "# Configure tree-based imputer\n",
    "ets = ExtraTreesRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "imputer = IterativeImputer(\n",
    "    estimator=ets,\n",
    "    max_iter=2,\n",
    "    tol=1e-3,\n",
    "    initial_strategy=\"median\",\n",
    "    imputation_order=\"ascending\",\n",
    "    add_indicator=False,  # we created our own flags\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Fit on full covariate context (schema vars only; outcome-free)\n",
    "clean_x_cols = [c for c in all_x_cols if c not in cols_to_impute]\n",
    "cols_for_model = clean_x_cols + cols_to_impute\n",
    "\n",
    "X_full_context = df_analysis_sample[cols_for_model].copy()\n",
    "imputer.fit(X_full_context)\n",
    "X_imputed_array = imputer.transform(X_full_context)\n",
    "\n",
    "X_imputed = pd.DataFrame(\n",
    "    X_imputed_array,\n",
    "    columns=cols_for_model,\n",
    "    index=df_analysis_sample.index,\n",
    ")\n",
    "\n",
    "# Overwrite only the columns we chose to impute\n",
    "df_analysis_sample[cols_to_impute] = X_imputed[cols_to_impute]\n",
    "\n",
    "# Clip continuous variables to [1st, 99th] percentile to tame extreme imputations\n",
    "for col in col_schema[\"continuous\"]:\n",
    "    if col in cols_to_impute:\n",
    "        low_cap = df_analysis_sample[col].quantile(0.01)\n",
    "        high_cap = df_analysis_sample[col].quantile(0.99)\n",
    "        df_analysis_sample[col] = df_analysis_sample[col].clip(lower=low_cap, upper=high_cap)\n",
    "        print(f\"Clipped {col} to [{low_cap:.2f}, {high_cap:.2f}]\")\n",
    "\n",
    "# Sanity checks\n",
    "remaining_nans = df_analysis_sample[all_x_cols].isna().sum().sum()\n",
    "print(f\"\\nRemaining missing values in X schema vars: {remaining_nans}\")\n",
    "assert remaining_nans == 0, \"Imputation failed – some X covariates still contain NA.\"\n",
    "\n",
    "for col in missing_flag_cols:\n",
    "    unique_vals = set(df_analysis_sample[col].dropna().unique())\n",
    "    if not unique_vals.issubset({0, 1}):\n",
    "        raise ValueError(f\"Flag {col} has non-binary values: {unique_vals}\")\n",
    "\n",
    "print(\n",
    "    f\"Imputation successful: {df_analysis_sample.shape[0]} rows, \"\n",
    "    f\"{len(all_x_cols)} covariates + {len(missing_flag_cols)} missingness flags\"\n",
    ")\n",
    "\n",
    "final_covariates = all_x_cols + missing_flag_cols\n",
    "len(final_covariates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2205e",
   "metadata": {},
   "source": [
    "## 14. Final dataset and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns needed for downstream causal ML / econometrics\n",
    "core_cols = [\n",
    "    \"person_id\",\n",
    "    \"household_id\",\n",
    "    \"Z_lottery\",\n",
    "    \"W_medicaid\",\n",
    "    \"W_medicaid_months\",\n",
    "    \"weight_12m\",\n",
    "    \"weight_attrition\",\n",
    "]\n",
    "\n",
    "# Ensure we only keep columns that exist\n",
    "core_cols = [c for c in core_cols if c in df_analysis_sample.columns]\n",
    "\n",
    "final_y_cols = [c for c in Y_cols if c in df_analysis_sample.columns] + [\n",
    "    \"Y_income_num_12m\",\n",
    "    \"Y_catastrophic_exp_12m\",\n",
    "]\n",
    "\n",
    "final_x_cols = [c for c in final_covariates if c in df_analysis_sample.columns]\n",
    "\n",
    "all_keep = core_cols + final_y_cols + final_x_cols\n",
    "all_keep = list(dict.fromkeys(all_keep))  # de-duplicate while preserving order\n",
    "\n",
    "df_final = df_analysis_sample[all_keep].copy()\n",
    "print(\n",
    "    f\"Final prepared dataset: {df_final.shape[0]:,} rows × {df_final.shape[1]} columns\"\n",
    ")\n",
    "\n",
    "out_path = DATA_DIR / \"ohie_full_intermediate_dataset.feather\"\n",
    "df_final.to_feather(out_path)\n",
    "out_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
