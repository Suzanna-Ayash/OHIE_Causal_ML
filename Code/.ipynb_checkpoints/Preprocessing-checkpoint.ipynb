{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd802cde-09a7-49be-a23c-ab823b5a54b2",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70beee33-3565-416d-8716-b4aa02c72131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9bc11-6a1a-468d-9b8d-bf4e3b3cdd93",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb16a31-f98c-4486-b153-7df44de1e7e9",
   "metadata": {},
   "source": [
    "### Part I: Outcome Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050d5c2-8ee0-4ffc-b725-0c17957b6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data_Used/\" \n",
    "\n",
    "# For 12m survey data\n",
    "# I will only pull columns for outcomes I need, no covariates as this is post-treatment\n",
    "col_keep_y = [\n",
    "    # --- A. Key & Filter Variables ---\n",
    "    'person_id',      # The unique ID to merge all files\n",
    "    'returned_12m',   # The essential filter. We will build our base file *only*\n",
    "                      # from rows where this == 1 (or 'Yes').\n",
    "    'weight_12m',\n",
    "\n",
    "    # --- B. Primary Financial Outcomes (Y Variables) ---\n",
    "    # These variables directly measure financial hardship.\n",
    "    \n",
    "    # 1. Medical Debt\n",
    "    'cost_any_owe_12m',   # [1/0] Do they have *any* medical debt?\n",
    "    'cost_tot_owe_12m',   # [Num] How much medical debt do they have?\n",
    "\n",
    "    # 2. Financial Distress\n",
    "    'cost_borrow_12m',    # [1/0] Did they have to skip bills or borrow?\n",
    "    'cost_refused_12m',   # [1/0] Were they refused care for non-payment?\n",
    "\n",
    "    # --- C. Outcome Construction & Secondary Analysis ---\n",
    "    # These variables let us build our \"catastrophic\" outcome and\n",
    "    # conduct more detailed \"for what\" analysis.\n",
    "    \n",
    "    'cost_tot_oop_12m',   # [Num] Total Out-of-Pocket spending. We need this to calculate\n",
    "    'cost_any_oop_12m',\n",
    "                          # catastrophic expenditures.\n",
    "    'hhinc_cat_12m',      # [Cat] Household income. This is the *other half* of the\n",
    "                          # catastrophic expenditure calculation (OOP / Income).\n",
    "                          \n",
    "    # 3. Detailed Spending (for secondary analysis)\n",
    "    'cost_doc_oop_12m',   # [Num] OOP spending on doctors\n",
    "    'cost_er_oop_12m',    # [Num] OOP spending on ER visits\n",
    "    'cost_rx_oop_12m',    # [Num] OOP spending on prescriptions (a key theoretical channel)\n",
    "    'cost_oth_oop_12m'    # [Num] OOP spending on other care\n",
    "]\n",
    "\n",
    "df_y = pd.read_stata(f\"{path}oregonhie_survey12m_vars.dta\", columns=col_keep_y, \\\n",
    "                       convert_categoricals=False, preserve_dtypes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26203de9-f4ca-44ae-af1a-af0c085c1caf",
   "metadata": {},
   "source": [
    "### Part 2: Covariate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f279d1-cb7b-4813-90d2-92714c5e058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 0m survey data\n",
    "# To pull covariates (X)\n",
    "\n",
    "col_keep_X = [\n",
    "    # --- A. Key & Filter Variables ---\n",
    "    'person_id',         # The unique ID to merge all files\n",
    "    'returned_0m',       # We will use this to confirm the row has baseline data\n",
    "    'surv_lang_0m',      # Language of survey, a potential control variable\n",
    "    \n",
    "    # --- B. Baseline Health Need & Utilization ---\n",
    "    # These variables are critical for our theory. People who are\n",
    "    # sicker or use more care at baseline may get the biggest\n",
    "    # financial protection from Medicaid.\n",
    "    'need_med_0m',       # [1/0] Did they need medical care in the last 6 mos?\n",
    "    'needmet_med_0m',    # [1/0] Did they get *all* the care they needed? (Captures unmet need)\n",
    "    'need_rx_0m',        # [1/0] Did they need prescription drugs?\n",
    "    'rx_num_mod_0m',     # [Num] How many *different* prescription drugs? (Measures chronic need)\n",
    "    'doc_num_mod_0m',    # [Num] Number of doctor visits in last 6 mos (Measures utilization)\n",
    "    'er_num_mod_0m',     # [Num] Number of ER visits in last 6 mos (Measures high-cost shocks)\n",
    "    'hosp_num_mod_0m',   # [Num] Number of hospitalizations in last 6 mos (Measures high-cost shocks)\n",
    "    'ins_months_0m',     # [Num] How many of the last 6 mos were they insured? (A key eligibility/need var)\n",
    "    \n",
    "    # --- C. Baseline Health Status ---\n",
    "    # Captures their general health *before* the lottery.\n",
    "    'health_gen_0m',     # [1-5] Overall health (Excellent, Good, Fair, Poor)\n",
    "    'baddays_phys_0m',   # [0-30] Num days physical health was \"not good\"\n",
    "    'baddays_ment_0m',   # [0-30] Num days mental health was \"not good\"\n",
    "    'health_chg_0m',     # [1-3] Health trajectory (Better, Same, Worse)\n",
    "\n",
    "    # --- D. Baseline Diagnosed Conditions ---\n",
    "    # Specific chronic conditions that have known financial implications.\n",
    "    'dia_dx_0m',         # [1/0] Diagnosed with Diabetes\n",
    "    'ast_dx_0m',         # [1/0] Diagnosed with Asthma\n",
    "    'hbp_dx_0m',         # [1/0] Diagnosed with High Blood Pressure\n",
    "    'emp_dx_0m',         # [1/0] Diagnosed with Emphysema/COPD\n",
    "    'chf_dx_0m',         # [1/0] Diagnosed with Congestive Heart Failure\n",
    "    'dep_dx_0m',         # [1/0] Diagnosed with Depression/Anxiety\n",
    "\n",
    "    # --- E. Baseline Demographics ---\n",
    "    'female_0m',         # [1/0] Gender variable\n",
    "    'birthyear_0m',      # [Year] We will use this to calculate baseline age\n",
    "    'edu_0m',            # [Cat] Highest level of education\n",
    "    \n",
    "    # --- F. Baseline Race & Ethnicity ---\n",
    "    'race_hisp_0m',      # [1/0] Hispanic\n",
    "    'race_white_0m',     # [1/0] White\n",
    "    'race_black_0m',     # [1/0] Black\n",
    "    'race_amerindian_0m',# [1/0] American Indian/Alaska Native\n",
    "    'race_asian_0m',     # [1/0] Asian\n",
    "    'race_pacific_0m',   # [1/0] Native Hawaiian/Pacific Islander\n",
    "    'race_other_qn_0m',  # [1/0] Other race\n",
    "    \n",
    "    # --- G. Baseline Employment & Household ---\n",
    "    'employ_hrs_0m',     # [Cat] Hours worked per week (Note: Corrected from 'emply_hrs_qn_0m')\n",
    "    'employ_0m',\n",
    "    'hhinc_cat_0m',      # [Cat] Household income category (CRITICAL covariate)\n",
    "    'hhsize_0m',         # [Num] Household size\n",
    "    'num19_0m',          # [Num] Number of children under 19 in household\n",
    "    \n",
    "    # --- H. Baseline Financial Status ---\n",
    "    # This is the baseline version of our outcome. It is a very powerful predictor.\n",
    "    'cost_tot_oop_correct_0m' # [Num] Total *corrected* out-of-pocket spending\n",
    "]\n",
    "\n",
    "df_X = pd.read_stata(f\"{path}oregonhie_survey0m_vars.dta\", columns=col_keep_X, \\\n",
    "                       convert_categoricals=False, preserve_dtypes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9ce00-2a64-4fd9-9ad3-0bf33477746b",
   "metadata": {},
   "source": [
    "### Part 3: Instrument Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d21116-7e43-43dd-8f50-33976f031056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For descriptive data\n",
    "# To pull instrument mainly\n",
    "col_keep_IV = [\n",
    "    # --- A. Key & Instrument Variables ---\n",
    "    'person_id',         # The unique ID to merge all files\n",
    "    'household_id',      # CRITICAL: We must have this to cluster our standard\n",
    "                         # [cite_start]errors, as the lottery was a household-level event[cite: 5429].\n",
    "    'treatment',         # [1/0] This is our Instrument (Z). [cite_start]1 = Won the lottery[cite: 5422].\n",
    "    \n",
    "    # --- B. Critical Control Variable ---\n",
    "    'numhh_list',        # [Num] Number of people in the household on the lottery list.\n",
    "                         # The original papers state we *must* control for this\n",
    "                         # in all analyses, as it affected the probability of\n",
    "                         # [cite_start]any single person winning[cite: 5427].\n",
    "                         \n",
    "    # --- C. Baseline Demographics (from lottery list) ---\n",
    "    # These are high-quality covariates since they were collected\n",
    "    # for everyone *before* the lottery.\n",
    "    'zip_msa_list'       # [1/0] Is the zip code in a Metropolitan Statistical Area (urban vs. rural).\n",
    "\n",
    "]\n",
    "\n",
    "df_IV = pd.read_stata(f\"{path}oregonhie_descriptive_vars.dta\", columns=col_keep_IV, \\\n",
    "                       convert_categoricals=False, preserve_dtypes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f875a52-a006-4099-ba2f-355539b327fc",
   "metadata": {},
   "source": [
    "### Part 4: Treatment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f774632-eb89-4c77-8e48-ae483f231d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For state program data\n",
    "# To pull treatment\n",
    "col_keep_W = [\n",
    "    # --- A. Key & Treatment Variables ---\n",
    "    'person_id',                       # The unique ID to merge all files\n",
    "    \n",
    "    'ohp_all_ever_firstn_30sep2009'    # [1/0] This is our Treatment (W).\n",
    "                                       # The codebook confirms this is the correct\n",
    "                                       # \"ever enrolled in Medicaid\" variable to use\n",
    "                                       # [cite_start]for the 12-month survey analysis [cite: 6535-6537].\n",
    "]\n",
    "\n",
    "df_W = pd.read_stata(f\"{path}oregonhie_stateprograms_vars.dta\", columns=col_keep_W, \\\n",
    "                       convert_categoricals=False, preserve_dtypes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87aed0-3e15-4887-8774-7fc29dfab4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded outcome data: {df_y.shape}\")\n",
    "print(f\"Loaded covariate data: {df_X.shape}\")\n",
    "print(f\"Loaded IV data: {df_IV.shape}\")\n",
    "print(f\"Loaded treatment data: {df_W.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62720b3c-fcea-43b3-a959-8db84e0d0b4a",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff3204-da76-41a9-8ff5-bd59358fdcb2",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550de4a2-2fae-42f4-a763-a015c0d29c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there are no duplicates\n",
    "def check_unique(df, name):\n",
    "    counts = df['person_id'].value_counts()\n",
    "    dups = counts[counts > 1]\n",
    "    if not dups.empty:\n",
    "        print(f\"[WARN] {name}: {len(dups)} duplicate person_id(s). Showing first 5:\")\n",
    "        print(dups.head())\n",
    "    else:\n",
    "        print(f\"[OK] {name} is 1 row per person_id\")\n",
    "\n",
    "check_unique(df_y, \"output data\")\n",
    "check_unique(df_X, \"covariate data\")\n",
    "check_unique(df_IV, \"instrument data\")\n",
    "check_unique(df_W, \"treatment data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b92722-3316-4d62-92d2-aeebf7da4d8c",
   "metadata": {},
   "source": [
    "#### Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d80c5-d10a-4e6c-a2f4-dbcc9168273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = (df_y\n",
    "             .merge(df_X, on='person_id', how='left')\n",
    "             .merge(df_IV, on='person_id', how='left')\n",
    "             .merge(df_W, on='person_id', how='left')\n",
    "            )\n",
    "print(f\"After merge: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642d8fe-9e10-4411-bfae-6800a60ef47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee313ac-4c7c-45da-87ac-58225fa75f81",
   "metadata": {},
   "source": [
    "#### Keeping only those who we have necessary information about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba609fc-995b-4803-92db-954c30583f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returned_0m\n",
    "print(df_merged['returned_0m'].value_counts())\n",
    "df_merged[df_merged['returned_0m'] == 0.0][col_keep_X] \\\n",
    "                .isna().sum()/len(df_merged[df_merged['returned_0m'] == 0.0])\n",
    "# output obviously shows we should only keep those with returned_0m == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbec214-a7fe-4c84-8e38-16022357c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returned_12m\n",
    "print(df_merged['returned_12m'].value_counts())\n",
    "df_merged[df_merged['returned_12m'] == 0.0][col_keep_y] \\\n",
    "                .isna().sum()/len(df_merged[df_merged['returned_12m'] == 0.0])\n",
    "# output obviously shows we should only keep those with returned_12m == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029b695-c249-4526-9e06-9bb9d70a7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those who did not answer 0-month survey or 12-month survey\n",
    "df_merged_full = df_merged[\n",
    "        (df_merged['returned_0m'] == 1.0) & \n",
    "        (df_merged['returned_12m'] == 1.0)].copy()\n",
    "print(f\"Responders to both surveys: {df_merged_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f0601-6252-4559-a058-06d16fbbf641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how treatment and instrument are distibuted among these reponders\n",
    "print(df_merged_full['treatment'].value_counts())\n",
    "print(df_merged_full['ohp_all_ever_firstn_30sep2009'].value_counts())\n",
    "# no nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba9901-bf8b-4af8-90f4-508b0ba587e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on fully merged respondants\n",
    "df_merged_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2068e-1858-4ebd-bbb0-4402393fbbce",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a445b6a-9f86-439d-be6e-279e7c7e9f58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Calculate Missing Counts & Percentages\n",
    "missing_report = df_merged_full.isnull().sum().to_frame(name='Missing_Count')\n",
    "missing_report['Percent_Missing'] = (missing_report['Missing_Count'] / len(df_merged_full)) * 100\n",
    "\n",
    "# 2. Filter to show only columns with ANY missing data\n",
    "missing_report = missing_report[missing_report['Missing_Count'] > 0].sort_values('Percent_Missing', ascending=False)\n",
    "\n",
    "# 3. Display\n",
    "print(f\"Total Analysis Sample Size: {len(df_merged_full)}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Variables with Missing Data:\")\n",
    "print(missing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d212d-53f7-4b3f-ae36-7d15b1d8f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If 'Any Debt' is NO (0), then 'Total Debt' is $0.\n",
    "print(df_merged_full[['cost_any_owe_12m', 'cost_tot_owe_12m']].value_counts()) #looks perfect, data clean\n",
    "\n",
    "mask_zero_debt = (df_merged_full['cost_any_owe_12m'] == 0) & (df_merged_full['cost_tot_owe_12m'].isna())\n",
    "df_merged_full.loc[mask_zero_debt, 'cost_tot_owe_12m'] = 0\n",
    "\n",
    "mask_zero_debt = (df_merged_full['cost_tot_owe_12m'] == 0) & (df_merged_full['cost_any_owe_12m'].isna())\n",
    "df_merged_full.loc[mask_zero_debt, 'cost_any_owe_12m'] = 0\n",
    "\n",
    "print(df_merged_full[['cost_any_owe_12m', 'cost_tot_owe_12m']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7bc51-74cb-46a8-b05a-db5da1db5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Inconsistencies\n",
    "# Condition: Respondent wrote \"$0\" (or 0.0) but checked \"Yes\"\n",
    "print(df_merged_full[['cost_any_oop_12m', 'cost_tot_oop_12m']].value_counts())\n",
    "\n",
    "mask_conflict = (df_merged_full['cost_tot_oop_12m'] == 0) & (df_merged_full['cost_any_oop_12m'] == 1)\n",
    "\n",
    "print(f\"Conflict Found: {mask_conflict.sum()} respondents reported $0 spending but checked 'Yes'.\")\n",
    "\n",
    "mask_conflict_2 = (df_merged_full['cost_tot_oop_12m'] > 0) & (df_merged_full['cost_any_oop_12m'] == 0)\n",
    "print(f\"Conflict Found: {mask_conflict_2.sum()} respondents reported > $0 spending but checked 'No'.\")\n",
    "\n",
    "# Logical Fix: Force 'Any' to 0 if 'Total' is $0. I trust the specific dollar declaration (Intensive Margin) \n",
    "# over the generic checkbox (Extensive Margin) to resolve measurement error\n",
    "df_merged_full.loc[mask_conflict, 'cost_any_oop_12m'] = 0\n",
    "new_mask_conflict = (df_merged_full['cost_tot_oop_12m'] == 0) & (df_merged_full['cost_any_oop_12m'] == 1)\n",
    "print(f\"Conflict Found (corrected): {new_mask_conflict.sum()} respondents reported $0 spending but checked 'Yes'.\")\n",
    "print(df_merged_full[['cost_any_oop_12m', 'cost_tot_oop_12m']].value_counts())\n",
    "\n",
    "mask_zero_oop = (df_merged_full['cost_tot_oop_12m'] == 0) & (df_merged_full['cost_any_oop_12m'].isna())\n",
    "df_merged_full.loc[mask_zero_debt, 'cost_any_oop_12m'] = 0\n",
    "\n",
    "mask_zero_oop = (df_merged_full['cost_any_oop_12m'] == 0) & (df_merged_full['cost_tot_oop_12m'].isna())\n",
    "df_merged_full.loc[mask_zero_debt, 'cost_tot_oop_12m'] = 0\n",
    "\n",
    "print(df_merged_full[['cost_any_oop_12m', 'cost_tot_oop_12m']].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f655b-9952-4294-8626-a850b3eb6f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "    'cost_doc_oop_12m', \n",
    "    'cost_er_oop_12m', \n",
    "    'cost_rx_oop_12m', \n",
    "    'cost_oth_oop_12m'\n",
    "]\n",
    "mask_zero_total = (df_merged_full['cost_tot_oop_12m'] == 0)\n",
    "for col in targets:\n",
    "    mask_fill = mask_zero_total & df_merged_full[col].isna()\n",
    "    \n",
    "    # Fill with 0\n",
    "    df_merged_full.loc[mask_fill, col] = 0\n",
    "    \n",
    "    print(f\"-> Filled {mask_fill.sum()} missing rows in '{col}' with 0.\")\n",
    "\n",
    "# 5. Verify AFTER\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54fd04-bca9-40cd-9859-a373121ca545",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_cols = [c for c in df_merged_full.columns if 'race_' in c]\n",
    "print(f\"Race columns found: {race_cols}\")\n",
    "race_sums = df_merged_full[race_cols].sum(axis=1)\n",
    "multiracial_count = (race_sums > 1).sum()\n",
    "print(f\"Check: {multiracial_count} respondents selected more than one race/ethnicity.\")\n",
    "mask_engaged = (df_merged_full[race_cols].sum(axis=1) >= 1)\n",
    "print(f\"Found {mask_engaged.sum()} respondents who selected a race.\")\n",
    "df_merged_full.loc[mask_engaged, race_cols] = df_merged_full.loc[mask_engaged, race_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3650f2-4e75-4777-ae70-7fab86130be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Proxy (Adults + Kids)\n",
    "# We fill missing kids with 0 so the addition works.\n",
    "proxy_size = df_merged_full['numhh_list'] + df_merged_full['num19_0m'].fillna(0)\n",
    "df_merged_full['hhsize_0m'] = df_merged_full['hhsize_0m'].fillna(proxy_size)\n",
    "print(f\"Missing Household Size AFTER: {df_merged_full['hhsize_0m'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd71009-beda-4288-b19f-d6b050347cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged_full[['employ_0m', 'employ_hrs_0m']].value_counts())\n",
    "mask_zero_emp= (df_merged_full['employ_0m'] == 0) & (df_merged_full['employ_hrs_0m'].isna())\n",
    "df_merged_full.loc[mask_zero_emp, 'employ_hrs_0m'] = 0\n",
    "\n",
    "mask_zero_emp = (df_merged_full['employ_hrs_0m'] == 0) & (df_merged_full['employ_0m'].isna())\n",
    "df_merged_full.loc[mask_zero_emp, 'employ_0m'] = 0\n",
    "\n",
    "print(df_merged_full[['employ_0m', 'employ_hrs_0m']].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943a9df-dcf4-4157-9583-85a85b206027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Missing Counts & Percentages\n",
    "missing_report = df_merged_full.isnull().sum().to_frame(name='Missing_Count')\n",
    "missing_report['Percent_Missing'] = (missing_report['Missing_Count'] / len(df_merged_full)) * 100\n",
    "\n",
    "# 2. Filter to show only columns with ANY missing data\n",
    "missing_report = missing_report[missing_report['Missing_Count'] > 0].sort_values('Percent_Missing', ascending=False)\n",
    "\n",
    "# 3. Display\n",
    "print(f\"Total Analysis Sample Size: {len(df_merged_full)}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Variables with Missing Data:\")\n",
    "print(missing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb60534-a08d-4cbd-aa21-d8298d237062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude IDs and things already cleaned (like surv_lang_0m)\n",
    "cols_to_impute = [\n",
    "    col for col in col_keep_X\n",
    "    if col not in ['person_id', 'returned_0m', 'surv_lang_0m']\n",
    "    and col in df_merged_full.columns\n",
    "]\n",
    "\n",
    "cols_to_impute = [\n",
    "    col for col in cols_to_impute\n",
    "    if np.issubdtype(df_merged_full[col].dtype, np.number)\n",
    "]\n",
    "\n",
    "print(f\"Imputing {len(cols_to_impute)} baseline covariates.\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Create missingness flags BEFORE imputation\n",
    "# ----------------------------------------------------\n",
    "missing_flag_cols = []\n",
    "for col in cols_to_impute:\n",
    "    if df_merged_full[col].isna().any():\n",
    "        flag_col = f\"{col}_missing\"\n",
    "        df_merged_full[flag_col] = df_merged_full[col].isna().astype(int)\n",
    "        missing_flag_cols.append(flag_col)\n",
    "        print(f\"✓ Created missing flag for {col}: \"\n",
    "              f\"{df_merged_full[col].isna().mean():.2%} missing\")\n",
    "\n",
    "print(f\"Total missing-flag columns created: {len(missing_flag_cols)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Clip extreme outliers in a few key covariates\n",
    "# ----------------------------------------------------\n",
    "# Only do this for pure covariates; do NOT clip outcomes here.\n",
    "if 'cost_tot_oop_correct_0m' in df_merged_full.columns:\n",
    "    cap = df_merged_full['cost_tot_oop_correct_0m'].quantile(0.99)\n",
    "    df_merged_full['cost_tot_oop_correct_0m'] = (\n",
    "        df_merged_full['cost_tot_oop_correct_0m'].clip(upper=cap)\n",
    "    )\n",
    "    print(f\"Clipped cost_tot_oop_correct_0m at 99th percentile = {cap:.2f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "#  Keep track of valid ranges for some ordinal variables (before imputation)\n",
    "# ----------------------------------------------------\n",
    "ordered_cat_cols = [\n",
    "    col for col in ['health_gen_0m', 'health_chg_0m', 'hhinc_cat_0m', 'edu_0m']\n",
    "    if col in cols_to_impute\n",
    "]\n",
    "\n",
    "orig_minmax = {\n",
    "    col: (df_merged_full[col].min(skipna=True), df_merged_full[col].max(skipna=True))\n",
    "    for col in ordered_cat_cols\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "#  Configure the IterativeImputer with ExtraTrees (missRanger-ish)\n",
    "# ----------------------------------------------------\n",
    "ets = ExtraTreesRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "imputer = IterativeImputer(\n",
    "    estimator=ets,\n",
    "    max_iter=7,\n",
    "    tol=1e-3,\n",
    "    initial_strategy='median',\n",
    "    imputation_order='ascending',\n",
    "    add_indicator=False,     # we created our own missing flags\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "#  Fit the imputer on X and transform\n",
    "# ----------------------------------------------------\n",
    "X_before = df_merged_full[cols_to_impute]\n",
    "\n",
    "print(\"Fitting IterativeImputer on baseline covariates X...\")\n",
    "X_imputed_array = imputer.fit_transform(X_before)\n",
    "X_imputed = pd.DataFrame(\n",
    "    X_imputed_array,\n",
    "    columns=cols_to_impute,\n",
    "    index=df_merged_full.index\n",
    ")\n",
    "\n",
    "df_merged_full[cols_to_impute] = X_imputed\n",
    "\n",
    "# ----------------------------------------------------\n",
    "#  Post-process integer / count / binary / ordinal columns\n",
    "# ----------------------------------------------------\n",
    "# Explicit lists are safer than substring heuristics\n",
    "binary_cols = [\n",
    "    c for c in [\n",
    "        'need_med_0m', 'needmet_med_0m', 'need_rx_0m',\n",
    "        'dia_dx_0m', 'ast_dx_0m', 'hbp_dx_0m', 'emp_dx_0m',\n",
    "        'chf_dx_0m', 'dep_dx_0m', 'female_0m', 'employ_det_0m',\n",
    "        'race_hisp_0m', 'race_white_0m', 'race_black_0m',\n",
    "        'race_amerindian_0m', 'race_asian_0m', 'race_pacific_0m',\n",
    "        'race_other_qn_0m'\n",
    "    ]\n",
    "    if c in cols_to_impute\n",
    "]\n",
    "\n",
    "count_cols = [\n",
    "    c for c in [\n",
    "        'rx_num_mod_0m', 'doc_num_mod_0m', 'er_num_mod_0m',\n",
    "        'hosp_num_0m', 'hosp_num_mod_0m',\n",
    "        'ins_months_0m', 'hhsize_0m', 'num19_0m'\n",
    "    ]\n",
    "    if c in cols_to_impute\n",
    "]\n",
    "\n",
    "# Binary: round to nearest and clip to {0, 1}\n",
    "for col in binary_cols:\n",
    "    df_merged_full[col] = df_merged_full[col].round()\n",
    "    df_merged_full[col] = df_merged_full[col].clip(0, 1)\n",
    "    df_merged_full[col] = df_merged_full[col].astype(int)\n",
    "    print(f\" → Cleaned binary {col} to 0/1\")\n",
    "\n",
    "# Counts: round and enforce non-negative\n",
    "for col in count_cols:\n",
    "    df_merged_full[col] = df_merged_full[col].round()\n",
    "    df_merged_full[col] = df_merged_full[col].clip(lower=0)\n",
    "    df_merged_full[col] = df_merged_full[col].astype(int)\n",
    "    print(f\" → Cleaned count {col} to integer ≥ 0\")\n",
    "\n",
    "# Ordinal categories: round and clip to original range\n",
    "for col in ordered_cat_cols:\n",
    "    low, high = orig_minmax[col]\n",
    "    df_merged_full[col] = df_merged_full[col].round()\n",
    "    df_merged_full[col] = df_merged_full[col].clip(low, high)\n",
    "    df_merged_full[col] = df_merged_full[col].astype(int)\n",
    "    print(f\" → Cleaned ordinal {col} to [{low}, {high}] integers\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Final validation\n",
    "# ----------------------------------------------------\n",
    "remaining_nans = df_merged_full[cols_to_impute].isna().sum().sum()\n",
    "print(f\"Imputation complete. Total missing values remaining in X: {remaining_nans}\")\n",
    "print(f\"Final X: {len(df_merged_full)} rows, \"\n",
    "      f\"{len(cols_to_impute)} imputed covariates + {len(missing_flag_cols)} missing flags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ac3a1-b01e-4ed1-8918-8a2954ffb892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Missing Counts & Percentages\n",
    "missing_report = df_merged_full.isnull().sum().to_frame(name='Missing_Count')\n",
    "missing_report['Percent_Missing'] = (missing_report['Missing_Count'] / len(df_merged_full)) * 100\n",
    "\n",
    "# 2. Filter to show only columns with ANY missing data\n",
    "missing_report = missing_report[missing_report['Missing_Count'] > 0].sort_values('Percent_Missing', ascending=False)\n",
    "\n",
    "# 3. Display\n",
    "print(f\"Total Analysis Sample Size: {len(df_merged_full)}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Variables with Missing Data:\")\n",
    "print(missing_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602a42d-9575-472d-aecf-03d090c5ed86",
   "metadata": {},
   "source": [
    "#### Fixing dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd135d-5747-43fb-95b5-f2a5066e7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix dtype of surv_lang_0m\n",
    "print(df_merged_full['surv_lang_0m'].value_counts())\n",
    "\n",
    "df_merged_full['surv_lang_0m'] = np.where(\n",
    "    df_merged_full['surv_lang_0m'] == 'English', 1, 0)\n",
    "\n",
    "print(df_merged_full['surv_lang_0m'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653b968-840c-436d-9048-cf9623eeb17f",
   "metadata": {},
   "source": [
    "#### Transforming Income from Cat to Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc3bcb-c7d8-4ec4-9a41-97d625dae9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPPING: OHIE Income Categories to Numeric Midpoints (2008 Dollars)\n",
    "\n",
    "# print(df_merged_full['hhinc_cat_12m'].value_counts())\n",
    "# print(df_merged_full['hhinc_cat_0m'].value_counts())\n",
    "\n",
    "income_map = {\n",
    "    1: 0,       # \"$0\"\n",
    "    2: 1250,    # \"$1 to $2,500\"\n",
    "    3: 3750,    # \"$2,501 to $5,000\"\n",
    "    4: 6250,    # \"$5,001 to $7,500\"\n",
    "    5: 8750,    # \"$7,501 to $10,000\"\n",
    "    6: 11250,   # \"$10,001 to $12,500\"\n",
    "    7: 13750,   # \"$12,501 to $15,000\"\n",
    "    8: 16250,   # \"$15,001 to $17,500\"\n",
    "    9: 18750,   # \"$17,501 to $20,000\"\n",
    "    10: 21250,  # \"$20,001 to $22,500\"\n",
    "    11: 23750,  # \"$22,501 to $25,000\"\n",
    "    12: 26250,  # \"$25,001 to $27,500\"\n",
    "    13: 28750,  # \"$27,501 to $30,000\"\n",
    "    14: 31250,  # \"$30,001 to $32,500\"\n",
    "    15: 33750,  # \"$32,501 to $35,000\"\n",
    "    16: 36250,  # \"$35,001 to $37,500\"\n",
    "    17: 38750,  # \"$37,501 to $40,000\"\n",
    "    18: 41250,  # \"$40,001 to $42,500\"\n",
    "    19: 43750,  # \"$42,501 to $45,000\"\n",
    "    20: 46250,  # \"$45,001 to $47,500\"\n",
    "    21: 48750,  # \"$47,501 to $50,000\"\n",
    "    22: 60000   # \"$50,001 or more\" does not matter\n",
    "}\n",
    "df_merged_full['income_num_12m'] = df_merged_full['hhinc_cat_12m'].map(income_map)\n",
    "df_merged_full['income_num_0m'] = df_merged_full['hhinc_cat_0m'].map(income_map)\n",
    "df_merged_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c21bad-7ce6-4acd-b2f4-9a35055dc506",
   "metadata": {},
   "source": [
    "#### Creating Catastrophic Expenditure Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec576a3-8d32-4256-b077-ed2438c84412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION: Catastrophic Expenditure = OOP > 30% of Income \n",
    "# Handle division by zero: If income is 0, any OOP > 0 is catastrophic.\n",
    "\n",
    "def calc_catastrophic(row):\n",
    "    income = row['income_num_12m']\n",
    "    oop = row['cost_tot_oop_12m']\n",
    "    \n",
    "    # Missing data check\n",
    "    if pd.isna(income) or pd.isna(oop):\n",
    "        return np.nan\n",
    "        \n",
    "    if income == 0:\n",
    "        return 1 if oop > 0 else 0\n",
    "    \n",
    "    return 1 if (oop / income) > 0.30 else 0\n",
    "\n",
    "df_merged_full['catastrophic_exp_12m'] = df_merged_full.apply(calc_catastrophic, axis=1)\n",
    "\n",
    "print(f\"Catastrophic Rate: {df_merged_full['catastrophic_exp_12m'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02922a1f-a915-4f93-93f6-1b3cac3932ba",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e84910-16b0-48a5-8d18-019ffd824658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_full['X_age_0m'] = 2008 - df_merged_full['birthyear_0m']\n",
    "print(f\"Age Range: {df_merged_full['X_age_0m'].min()} to {df_merged_full['X_age_0m'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33072d-706c-4edc-bd43-39f0d46b6036",
   "metadata": {},
   "source": [
    "#### OHE Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7283c-702f-4e2c-8b99-4fa5a9a820af",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_dummies = pd.get_dummies(df_merged_full['employ_det_0m'], prefix='X_employ', dummy_na=False)\n",
    "df_merged_full = pd.concat([df_merged_full, emp_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e739ce1-cb74-4bf1-8e0c-8519d0a4224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_full['employ_det_0m'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6bb248-3235-41d0-b87c-0510ed6f70cf",
   "metadata": {},
   "source": [
    "#### Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20abb3c-fa7e-4826-b0f3-aa834418f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_vars = [\n",
    "    'cost_tot_oop_correct_0m', \n",
    "    'doc_num_mod_0m', \n",
    "    'er_num_mod_0m', \n",
    "    'hosp_num_mod_0m',\n",
    "    'rx_count_0m'\n",
    "]\n",
    "\n",
    "for col in continuous_vars:\n",
    "    if col in df_merged_full.columns:\n",
    "        p99 = df_merged_full[col].quantile(0.99)\n",
    "        cutoff = 2 * p99\n",
    "        \n",
    "        outliers = (df_merged_full[col] > cutoff).sum()\n",
    "        \n",
    "        if outliers > 0:\n",
    "            # Cap the values in place\n",
    "            df_merged_full[col] = df_merged_full[col].clip(upper=cutoff)\n",
    "            print(f\" -> {col}: Capped {outliers} outliers at {cutoff:.2f} (p99={p99:.2f})\")\n",
    "        else:\n",
    "            print(f\" -> {col}: No outliers found above {cutoff:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c49706-d6db-4717-8a74-2fcd5b4ed177",
   "metadata": {},
   "source": [
    "#### Column Ranaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596e922-4b3b-4931-9a6b-ec9a8335bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_X = {col: f\"X_{col}\" for col in col_keep_X if col in df_merged_full.columns and col != 'person_id'}\n",
    "rename_Y = {col: f\"Y_{col}\" for col in col_keep_y if col in df_merged_full.columns and col != 'person_id'}\n",
    "special_renames = {\n",
    "    'treatment': 'Z_lottery',\n",
    "    'ohp_all_ever_firstn_30sep2009': 'W_medicaid',\n",
    "    'weight_12m': 'sample_weight'\n",
    "}\n",
    "\n",
    "df_merged_full.rename(columns=rename_X, inplace=True)\n",
    "df_merged_full.rename(columns=rename_Y, inplace=True)\n",
    "df_merged_full.rename(columns=special_renames, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7829d0-b302-4841-9b19-de55765166b5",
   "metadata": {},
   "source": [
    "#### Drop redundant or no longer needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3fbc7-cee8-4c64-938e-41bfc1bef42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_full = df_merged_full.drop(['hhinc_cat_0m', 'hhinc_cat_12m', \n",
    "                                      'returned_0m', 'returned_12m', \n",
    "                                      'birthyear_0m'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab806901-bbb2-48f6-8ed9-412b87623cd0",
   "metadata": {},
   "source": [
    "## Save data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406572d-3d0a-4891-a50e-05babebd6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final, clean dataset to a file\n",
    "file_name = \"../Data_Used/ohie_full_initial_dataset.feather\"\n",
    "df_merged_full.to_feather(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
